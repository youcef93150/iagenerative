"""
IntÃ©gration de l'IA GÃ©nÃ©rative (Gemini)
EF4: Augmentation par GenAI (StratÃ©gique et LimitÃ©e)

Contraintes obligatoires respectÃ©es:
- Appels API strictement limitÃ©s
- Un seul appel pour le plan de dÃ©couverte
- Un seul appel pour la bio/synthÃ¨se
- Caching automatique
- Enrichissement conditionnel uniquement

Architecture RAG:
- Retrieval: Fait par le moteur NLP
- Augmented Context: Construction du contexte enrichi ici
- Generation: Production via Gemini
"""

import os
from typing import List, Dict, Optional
import logging
import google.generativeai as genai
from dotenv import load_dotenv
from src.cache_manager import CacheManager

# Charger les variables d'environnement
load_dotenv()

logger = logging.getLogger(__name__)


class GenAIIntegration:
    """
    IntÃ©gration de l'IA gÃ©nÃ©rative Gemini (EF4)
    
    Respecte l'architecture RAG et toutes les contraintes de limitation des appels.
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        model_name: str = "gemini-2.0-flash-exp",
        cache_enabled: bool = True,
        max_cache_size: int = 100
    ):
        """
        Initialise l'intÃ©gration Gemini
        
        Args:
            api_key: ClÃ© API Gemini (ou depuis .env)
            model_name: Nom du modÃ¨le Gemini Ã  utiliser
            cache_enabled: Activer le caching
            max_cache_size: Taille max du cache
        """
        # RÃ©cupÃ©rer la clÃ© API
        self.api_key = api_key or os.getenv('GEMINI_API_KEY')
        
        if not self.api_key:
            raise ValueError(
                "âŒ ClÃ© API Gemini manquante. "
                "DÃ©finissez GEMINI_API_KEY dans .env ou passez-la en paramÃ¨tre."
            )
        
        # Configurer Gemini
        genai.configure(api_key=self.api_key)
        self.model_name = model_name
        self.model = genai.GenerativeModel(model_name)
        
        # Initialiser le cache (CONTRAINTE OBLIGATOIRE)
        self.cache = CacheManager(
            cache_dir=".cache",
            max_size=max_cache_size,
            enabled=cache_enabled
        )
        
        # Compteur d'appels (pour monitoring)
        self.api_calls_count = 0
        
        logger.info(f"âœ… GenAI initialisÃ© - ModÃ¨le: {model_name}, Cache: {cache_enabled}")
    
    def _call_gemini(self, prompt: str, use_cache: bool = True) -> str:
        """
        Appelle l'API Gemini avec gestion du cache
        
        Args:
            prompt: Le prompt Ã  envoyer
            use_cache: Utiliser le cache ou non
            
        Returns:
            RÃ©ponse gÃ©nÃ©rÃ©e
        """
        # VÃ©rifier le cache d'abord
        if use_cache:
            cached_response = self.cache.get(prompt, model=self.model_name)
            if cached_response:
                logger.info("âœ… RÃ©ponse rÃ©cupÃ©rÃ©e du cache (0 appel API)")
                return cached_response
        
        # Appel API
        try:
            logger.info(f"ðŸ“¡ Appel API Gemini #{self.api_calls_count + 1}")
            response = self.model.generate_content(prompt)
            result = response.text
            
            # Mettre en cache
            if use_cache:
                self.cache.set(prompt, result, model=self.model_name)
            
            self.api_calls_count += 1
            logger.info(f"âœ… RÃ©ponse gÃ©nÃ©rÃ©e (longueur: {len(result)} caractÃ¨res)")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors de l'appel API Gemini: {e}")
            return f"[Erreur de gÃ©nÃ©ration: {str(e)}]"
    
    def enrich_short_text(self, text: str, min_words: int = 15) -> tuple[str, bool]:
        """
        EF4.1: Enrichissement conditionnel de texte court (OPTIONNEL)
        
        N'appelle l'API QUE si le texte est trop court.
        
        Args:
            text: Texte Ã  Ã©ventuellement enrichir
            min_words: Nombre minimum de mots
            
        Returns:
            (texte_enrichi, was_enriched): Texte final et boolÃ©en d'enrichissement
        """
        word_count = len(text.split())
        
        # Si le texte est suffisant, ne pas enrichir
        if word_count >= min_words:
            logger.info(f"âœ… Texte suffisant ({word_count} mots) - Pas d'enrichissement")
            return text, False
        
        logger.info(f"âš ï¸ Texte court ({word_count} mots) - Enrichissement via GenAI")
        
        prompt = f"""Tu es un assistant qui enrichit des descriptions de prÃ©fÃ©rences cinÃ©matographiques.

Description courte de l'utilisateur : "{text}"

TÃ¢che : Enrichis cette description en ajoutant du contexte technique et des dÃ©tails sur :
- Les thÃ¨mes cinÃ©matographiques possibles
- L'atmosphÃ¨re recherchÃ©e
- Le style narratif qui pourrait correspondre

RÃ¨gles :
- Reste fidÃ¨le Ã  l'intention originale
- Ajoute 2-3 phrases maximum
- Utilise un ton naturel
- Ne change pas les prÃ©fÃ©rences exprimÃ©es, ajoute seulement du contexte

Description enrichie :"""
        
        enriched = self._call_gemini(prompt, use_cache=True)
        
        # Combiner le texte original avec l'enrichissement
        final_text = f"{text}\n\n{enriched.strip()}"
        
        logger.info(f"âœ… Texte enrichi ({len(final_text.split())} mots)")
        
        return final_text, True
    
    def generate_discovery_plan(
        self,
        weak_genres: List[str],
        recommendations: List[Dict],
        user_profile_summary: str
    ) -> str:
        """
        EF4.2: GÃ©nÃ©ration du Plan de DÃ©couverte (UN SEUL APPEL API)
        
        Ã‰quivalent AISCA: Plan de progression personnalisÃ©
        
        Args:
            weak_genres: Genres faiblement couverts (Ã  explorer)
            recommendations: Top 3 films recommandÃ©s
            user_profile_summary: RÃ©sumÃ© du profil utilisateur
            
        Returns:
            Plan de dÃ©couverte personnalisÃ©
        """
        logger.info("ðŸŽ¨ GÃ©nÃ©ration du plan de dÃ©couverte (1 appel API)")
        
        # Construction du contexte enrichi (AUGMENTED CONTEXT - RAG)
        reco_text = "\n".join([
            f"- {r['titre']} ({r['annee']}) de {r['realisateur']} - Score: {r['score_final']:.2f}"
            for r in recommendations[:3]
        ])
        
        weak_genres_text = ", ".join(weak_genres[:5]) if weak_genres else "Aucun"
        
        prompt = f"""Tu es un conseiller cinÃ©matographique expert qui crÃ©e des plans de dÃ©couverte personnalisÃ©s.

PROFIL UTILISATEUR :
{user_profile_summary}

FILMS RECOMMANDÃ‰S (Top 3) :
{reco_text}

GENRES Ã€ EXPLORER (faible affinitÃ© actuelle) :
{weak_genres_text}

TÃ‚CHE : CrÃ©e un plan de dÃ©couverte cinÃ©matographique personnalisÃ© incluant :

1. **Prochaines Ã‰tapes** : 3-4 films Ã  dÃ©couvrir en prioritÃ© (en dehors du top 3) pour enrichir le profil
2. **Genres Ã  Explorer** : Pourquoi dÃ©couvrir les genres faiblement couverts et films recommandÃ©s par genre
3. **Parcours ThÃ©matique** : Une progression logique (ex: du plus accessible au plus expÃ©rimental)

Ton : Enthousiaste, pÃ©dagogique, personnalisÃ©
Format : Markdown avec sections claires
Longueur : 300-400 mots maximum

Plan de DÃ©couverte :"""
        
        plan = self._call_gemini(prompt, use_cache=True)
        
        logger.info("âœ… Plan de dÃ©couverte gÃ©nÃ©rÃ©")
        
        return plan.strip()
    
    def generate_cinephile_profile(
        self,
        recommendations: List[Dict],
        genre_weights: Dict[str, float],
        mood_weights: Dict[str, float],
        coverage_score: float
    ) -> str:
        """
        EF4.3: SynthÃ¨se de Profil CinÃ©phile (UN SEUL APPEL API)
        
        Ã‰quivalent AISCA: Bio professionnelle (Executive Summary)
        
        Args:
            recommendations: Top 3 films
            genre_weights: Poids des genres prÃ©fÃ©rÃ©s
            mood_weights: Poids des moods prÃ©fÃ©rÃ©s
            coverage_score: Score de couverture global
            
        Returns:
            Profil cinÃ©phile personnalisÃ©
        """
        logger.info("ðŸŽ­ GÃ©nÃ©ration du profil cinÃ©phile (1 appel API)")
        
        # Identifier les genres prÃ©fÃ©rÃ©s (score > 0.7)
        top_genres = [g for g, w in sorted(genre_weights.items(), key=lambda x: x[1], reverse=True) if w > 0.7][:3]
        
        # Identifier les moods prÃ©fÃ©rÃ©s
        top_moods = [m for m, w in sorted(mood_weights.items(), key=lambda x: x[1], reverse=True) if w > 0.7][:3]
        
        # Films recommandÃ©s
        reco_titles = [f"{r['titre']} ({r['annee']})" for r in recommendations[:3]]
        
        prompt = f"""Tu es un expert en profils cinÃ©matographiques qui rÃ©dige des synthÃ¨ses personnalisÃ©es.

DONNÃ‰ES DU PROFIL :
- Genres prÃ©fÃ©rÃ©s : {', '.join(top_genres) if top_genres else 'VariÃ©'}
- Ambiances recherchÃ©es : {', '.join(top_moods) if top_moods else 'VariÃ©'}
- Films recommandÃ©s : {', '.join(reco_titles)}
- Score d'affinitÃ© global : {coverage_score:.2f}/1.00

TÃ‚CHE : RÃ©dige un profil cinÃ©phile personnalisÃ© (style executive summary) qui :

1. RÃ©sume les goÃ»ts cinÃ©matographiques de la personne
2. Identifie sa "signature" de cinÃ©phile (qu'est-ce qui caractÃ©rise ses choix ?)
3. Mentionne les rÃ©alisateurs ou mouvements qui pourraient l'intÃ©resser
4. Termine par une phrase accrocheuse qui capture son essence de spectateur

Ton : Professionnel mais chaleureux, prÃ©cis, valorisant
Format : Un seul paragraphe fluide
Longueur : 150-200 mots maximum

Profil CinÃ©phile :"""
        
        profile = self._call_gemini(prompt, use_cache=True)
        
        logger.info("âœ… Profil cinÃ©phile gÃ©nÃ©rÃ©")
        
        return profile.strip()
    
    def generate_film_justification(
        self,
        film: Dict,
        user_description: str,
        score_components: Dict[str, float]
    ) -> str:
        """
        GÃ©nÃ¨re une justification personnalisÃ©e pour une recommandation
        
        Args:
            film: Dictionnaire du film recommandÃ©
            user_description: Description originale de l'utilisateur
            score_components: Composantes du score (sÃ©mantique, genre, mood)
            
        Returns:
            Justification de la recommandation
        """
        prompt = f"""Explique en 2-3 phrases pourquoi le film "{film['titre']}" ({film['annee']}) 
correspond aux prÃ©fÃ©rences de l'utilisateur.

PrÃ©fÃ©rences utilisateur : {user_description[:200]}...

Description du film : {film['description'][:300]}...

Scores :
- SimilaritÃ© sÃ©mantique : {score_components['sÃ©mantique']:.2f}
- AffinitÃ© genre : {score_components['genre']:.2f}
- AffinitÃ© mood : {score_components['mood']:.2f}

Justification concise et personnalisÃ©e :"""
        
        justification = self._call_gemini(prompt, use_cache=True)
        
        return justification.strip()
    
    def get_api_stats(self) -> Dict:
        """
        Retourne les statistiques d'utilisation de l'API
        
        Returns:
            Dictionnaire avec les stats
        """
        cache_stats = self.cache.get_stats()
        
        return {
            "api_calls_count": self.api_calls_count,
            "cache_stats": cache_stats,
            "model_name": self.model_name
        }
